{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7918b407-9f08-476e-854c-23edabdac5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "# K-nearest neighbor\n",
    "# PCA\n",
    "# linear regression\n",
    "# logistic regression\n",
    "# SVM\n",
    "# decision tree\n",
    "# Sampling\n",
    "## stratified sampling\n",
    "## uniform sampling\n",
    "## reservoir sampling\n",
    "## sampling multinomial distribution\n",
    "## random generator\n",
    "# NLP algorithms (if that's your area of work)\n",
    "## bigrams\n",
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39a1acc-9453-4abb-85c6-755968b8c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "# K-means clustering\n",
    "import numpy as np\n",
    "def kMeansClustering(X, K):\n",
    "    Cx = np.random.random_sample(K)*(np.max(X)- np.min(X)) + np.min(X)\n",
    "    Cy = np.random.random_sample(K)*(np.max(X)- np.min(X)) + np.min(X)\n",
    "    C = np.array(list(zip(Cx, Cy)), dtype=np.float32) # numpy array is faster than python list\n",
    "    print(np.shape(C))\n",
    "\n",
    "    for i in range(100):\n",
    "        m = len(X)\n",
    "        idx = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            dist = np.linalg.norm(X[i] - C, axis = 1)\n",
    "            idx[i] = np.argmin(dist)\n",
    "\n",
    "        # update the position of the K centroids\n",
    "        for i in range(K):\n",
    "            points = [X[j] for j in range(m) if idx[j] == i]\n",
    "            C[i] = np.mean(points, axis = 0)\n",
    "    return C\n",
    "\n",
    "arrays = [[12, 39], [20, 36], [28, 30], [18, 52], [29, 54], [33, 46], [24, 55],\n",
    "[45, 59], [45, 63], [52, 70], [51, 66], [52, 63], [55, 58], [53, 23], [55, 14],\n",
    "[61, 8], [64, 19], [69, 7], [72, 24]]\n",
    "C = kMeansClustering(arrays,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82dc37bb-881a-4557-bb71-98ec15e1f941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ceea5e5cc8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [x[0] for x in arrays]\n",
    "y = [x[1] for x in arrays]\n",
    "Cx = [x[0] for x in C]\n",
    "Cy = [x[1] for x in C]\n",
    "plt.scatter(x, y, color='g')\n",
    "plt.scatter(Cx, Cy, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a21ce70-a584-4c95-86e7-a45f115ea228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zqian\\AppData\\Roaming\\Python\\Python37\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PCA\n",
    "## Scale the data — we don’t want some feature to be voted as “more important” due to scale differences.\n",
    "## Calculate covariance matrix — square matrix giving the covariances between each pair of elements of a random vector\n",
    "## Eigendecomposition\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Cx = np.random.random_sample(30)*4 + 10\n",
    "Cy = np.random.random_sample(30)*40 + 10\n",
    "Cz = np.random.random_sample(30)*100 + 10\n",
    "X = np.array(list(zip(Cx, Cy, Cz)), dtype=np.float32) # numpy array is faster than python list\n",
    "    \n",
    "X_scaled = StandardScaler().fit_transform(X) #30x3\n",
    "features = X_scaled.T # 3x30\n",
    "cov_matrix = np.cov(features) # 3x3\n",
    "values, vectors = np.linalg.eig(cov_matrix) # eigen decomposition, results ordered by variance of vectors(3x3)\n",
    "\n",
    "explained_variances = []\n",
    "for i in range(len(values)):\n",
    "    explained_variances.append(values[i]/np.sum(values))\n",
    "\n",
    "PC1 = np.dot(X_scaled, vectors.T[0]) # 30x3, 3x1  ---> 30x1\n",
    "PC2 = np.dot(X_scaled, vectors.T[1])\n",
    "sns.scatterplot(PC1, PC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ea4f447-afcd-49f6-927d-c774391fcc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(PC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb5622e9-ae0b-4f5f-bcfb-21c77d139197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46027938,  0.60291617,  0.65164023])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e41a102f-3763-419f-8543-6e0d25fc00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "# least square coefficient estimate\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.m = 0\n",
    "        self.b = 0\n",
    "    def fit(self, X, y):\n",
    "        self.m = sum((X-np.mean(X))*(y-np.mean(y)))/sum((X-np.mean(X))**2)\n",
    "        self.b = np.mean(y) - self.m*np.mean(X)\n",
    "    def predict(self, X): \n",
    "        return self.m*X + self.b\n",
    "    def coef(self, y, y_hat):\n",
    "        return 1 - sum((y-y_hat)**2)/sum((y-np.mean(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06b81506-2fd2-4f8a-8b0d-19bfd6372fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.992412056956919 -1.911093353240874\n",
      "0.9999626705987037\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(0,10,10)\n",
    "m, b  = 3, -2\n",
    "Y = m * X + b + 0.1 * np.random.randn(X.shape[0])\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,Y)\n",
    "Y_hat = lr.predict(X)\n",
    "R2 = lr.coef(Y,Y_hat)\n",
    "\n",
    "print(lr.m, lr.b)\n",
    "print(R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8eaa9b93-4d75-4f1f-9ca9-e94bdf65da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, learning_rate, max_iteration):\n",
    "        self.theta = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iteration = max_iteration\n",
    "    \n",
    "    # method\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1+np.exp(-z))\n",
    "    \n",
    "    def cost_func(self, X, y): # both h and y are 1D array\n",
    "        h = self.sigmoid(np.dot(X, self.theta))\n",
    "        J = 1.0/m *np.sum(-y*np.log(h) - (1.0-y)*np.log(1.0-h))\n",
    "        return J\n",
    "    \n",
    "    def gradients(self, X, y):\n",
    "        h = self.sigmoid(np.dot(X, self.theta))\n",
    "        return 1.0/m * np.dot(X.T, (h-y))\n",
    "    \n",
    "    def fit(self, X, y): # X is mxn dimension, m=sample size, n=feature dimension\n",
    "        cost = []\n",
    "        X = np.array([[1] + x for x in X]) # add x0=1 to all X\n",
    "        self.theta = np.random.rand(X.shape[1]) # array with n elements\n",
    "        for it in range(self.max_iteration):\n",
    "            cost.append(self.cost_func(X, y))\n",
    "            grads = self.gradients(X, y)\n",
    "            self.theta -= self.learning_rate*grads\n",
    "        print(cost)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        pred = self.sigmoid(np.dot(X, self.theta))\n",
    "        return pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa3620bf-0ee5-43d8-8fe0-128b112cc733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.049035790518848515, 0.048117772958766125, 0.047233481019227794, 0.04638109384600978, 0.04555891907947929, 0.04476538173582384, 0.043999014221549865, 0.043258447348895335, 0.04254240223717429, 0.04184968299989751, 0.04117917013024033, 0.04052981450835952, 0.0399006319634831, 0.03929069833183019, 0.038699144958461974, 0.03812515459727937, 0.037567957668688395, 0.03702682883909614, 0.0365010838904334, 0.03599007685144568, 0.035493197365588314, 0.03500986827309392, 0.034539543387168456, 0.03408170544639312, 0.03363586422726836, 0.03320155480249186, 0.032778335932017996, 0.03236578857525193, 0.031963514513878265, 0.03157113507584996, 0.031188289951983826, 0.030814636097421095, 0.030449846710941537, 0.030093610285776007, 0.029745629726146236, 0.029405621524286503, 0.029073314993175337, 0.028748451550629925, 0.02843078405079887, 0.028120076159436604, 0.02781610176965015, 0.027518644455097236, 0.027227496957864573, 0.026942460708489633, 0.026663345375799168, 0.026389968444425056, 0.026122154818036036, 0.025859736446477405, 0.025602551975157224, 0.02535044641514338, 0.025103270832565194, 0.024860882056007894, 0.024623142400703346, 0.024389919408398054, 0.02416108560187322, 0.023936518253161305, 0.023716099164576315, 0.023499714461741705, 0.02328725439785369, 0.023078613168475745, 0.02287368873621193, 0.022672382664647165, 0.02247459996098896, 0.022280248926884447, 0.02208924101692271, 0.02190149070436074, 0.02171691535365137, 0.021535435099372016, 0.021356972731182547, 0.02118145358446722, 0.02100880543633244, 0.02083895840666067, 0.02067184486393402, 0.020507399335561387, 0.020345558422461045, 0.020186260717664866, 0.020029446728724094, 0.019875058803712656, 0.019723041060634307, 0.019573339320052056, 0.019425901040770497, 0.019280675258409572, 0.019137612526721852, 0.01899666486150814, 0.01885778568700131, 0.01872092978459034, 0.018586053243767806, 0.018453113415188252, 0.018322068865732996, 0.018192879335481475, 0.018065505696495857, 0.01793990991332986, 0.017816055005179375, 0.017693905009594703, 0.017573424947679973, 0.017454580790709877, 0.017337339428095927, 0.017221668636640028, 0.01710753705101405, 0.016994914135410974]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.randn(5,3) + 5\n",
    "x2 = np.random.randn(5,3) - 5\n",
    "X = np.concatenate([x1,x2], axis=0)    \n",
    "y  = np.concatenate([np.ones(5), -np.zeros(5)], axis=0)   \n",
    "lr = LogisticRegression(learning_rate=0.01, max_iteration=100)\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a12f5e76-ef67-464f-9aff-6ff2045c1922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99691285e-01, 9.99766336e-01, 9.99551114e-01, 9.99710488e-01,\n",
       "       9.99443771e-01, 1.03324231e-03, 5.73465980e-04, 1.23758232e-03,\n",
       "       5.73662186e-03, 2.50268587e-03])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "42f39fcf-ca3f-477a-9974-7b741bb160b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "\n",
    "\n",
    "class SupportVectorMachine:\n",
    "    # constructor\n",
    "    def __init__(self, C, learning_rate, max_iteration):\n",
    "        self.weights = None\n",
    "        self.C = C\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iteration = max_iteration\n",
    "        \n",
    "    def cost_func(self, W, X, y):\n",
    "        # compute the hinge loss\n",
    "        N = X.shape[0]\n",
    "        distances = 1 - y*(np.dot(X, W))\n",
    "        distances[distances<0] = 0 # equivalent to max(0, distance)\n",
    "        hinge_loss = self.C * (np.sum(distances)/N)\n",
    "        cost = 1/2*np.dot(W, W) + hinge_loss\n",
    "        return cost\n",
    "\n",
    "    def gradients(self, W, X, y):\n",
    "        distances = 1 - y*np.dot(X, W)\n",
    "        gradients = np.zeros(len(W))\n",
    "        for idx, d in enumerate(distances):\n",
    "            if max(0, d) == 0:\n",
    "                gradients += W\n",
    "            else:\n",
    "                gradients += W - self.C * Y[idx] * X[idx]\n",
    "        return gradients/len(y)\n",
    "    \n",
    "    def fit(self, X, y): # X is mxn dimension, m=sample size, n=feature dimension\n",
    "        cost = []\n",
    "        X = np.array([[1] + x for x in X]) # add x0=1 to all X\n",
    "        y = [1 if y_ == 1 else -1 for y_ in y ]\n",
    "        self.weights = np.random.rand(X.shape[1]) # array with n elements\n",
    "        for it in range(self.max_iteration):\n",
    "            cost.append(self.cost_func(self.weights, X, y))\n",
    "            grads = self.gradients(self.weights, X, y)\n",
    "            self.weights -= self.learning_rate*grads\n",
    "        print(cost)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        f_x = np.dot(X, self.weights)\n",
    "        pred = [1 if f_x_ >= 0 else 0 for f_x_ in f_x]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad815c7b-e633-4cdd-9ee5-29d8d177c4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42149337610506593, 0.4131056579205751, 0.40488485532795565, 0.3968276467069293, 0.3889307765374614, 0.38119105408436593, 0.37360535210808704, 0.3661706056011361, 0.35888381054967355, 0.35174202271973504, 0.3447423564676123, 0.3378819835739068, 0.3311581321007861, 0.32456808527198044, 0.3181091803750681, 0.3117788076856042, 0.3055744094126607, 0.29949347866534876, 0.29353355843990836, 0.2876922406269542, 0.2819671650384778, 0.27635601845421215, 0.2708565336869734, 0.26546648866660255, 0.26018370554213716, 0.25500604980184866, 0.24993142941079188, 0.2449577939655171, 0.2400831338656033, 0.23530547950167774, 0.23062290045959435, 0.22603350474044845, 0.22153543799611355, 0.2171268827799909, 0.21280605781266906, 0.208571217262197, 0.20442065003867926, 0.20035267910290952, 0.19636566078876164, 0.19245798413906526, 0.18862807025469788, 0.18487437165662943, 0.18119537166066252, 0.1775895837646153, 0.17405555104769946, 0.17059184558185023, 0.16719706785477143, 0.16386984620446146, 0.1606088362649927, 0.1574127204233193, 0.15428020728689526, 0.15121003116188605, 0.14820095154176452, 0.1452517526060834, 0.14236124272922235, 0.1395282539989108, 0.1367516417443325, 0.13403028407362028, 0.13136308142055522, 0.12874895610028617, 0.12618685187389048, 0.12367573352160006, 0.12121458642452022, 0.11880241615467227, 0.1164382480731943, 0.11412112693653773, 0.11185011651050061, 0.10962429919194165, 0.10744277563802203, 0.10530466440282539, 0.10320910158120916, 0.10115524045974311, 0.09914225117459423, 0.09716932037621981, 0.09523565090073303, 0.09334046144780846, 0.09148298626499707, 0.08966247483832364, 0.087878191589041, 0.08612941557641908, 0.08441544020644835, 0.08273557294634003, 0.08108913504470786, 0.07947546125731818, 0.07789389957829754, 0.07634381097668942, 0.0748245691382533, 0.07333556021240206, 0.07187618256417527, 0.07044584653114817, 0.06904397418517833, 0.06766999909889329, 0.06632336611682532, 0.0650035311311005, 0.06370996086159159, 0.062442132640445916, 0.06119953420090103, 0.0599816634703031, 0.05878802836724406, 0.0576181466027359]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.randn(5,3) + 5\n",
    "x2 = np.random.randn(5,3) - 5\n",
    "X = np.concatenate([x1,x2], axis=0)    \n",
    "y  = np.concatenate([np.ones(5), -np.zeros(5)], axis=0)   \n",
    "svm = SupportVectorMachine(C=1, learning_rate=0.01,max_iteration=100)\n",
    "svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9427d647-56e3-43f3-a069-7c8a568081ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "44484024-43a4-4075-97ec-5c834edc4381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X1 < 6.642]\n",
      " [0]\n",
      " [1]\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def TreeNode(self, data = data, feature_index = None, split_val=None, left_child=None, right_child=None, class_value=None): # a treenode is defined by its split condition\n",
    "        self.data = data\n",
    "        self.feature_index = feature_index # feature index\n",
    "        self.split_val = split_val # only consider numerical val\n",
    "        self.left_child = left_child # TreeNode\n",
    "        self.right_child = right_child # TreeNode\n",
    "        self.class_value = class_value # only exist for terminal node\n",
    "        \n",
    "    # Split a dataset based on an attribute and an attribute value\n",
    "    def test_split(self, feature_index, value, dataset):\n",
    "        left_child, right_child = list(), list()\n",
    "        for row in dataset:\n",
    "            if row[feature_index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left_child, right_child\n",
    " \n",
    "    # Calculate the Gini index for a split dataset\n",
    "    def gini_index(self, groups, classes):\n",
    "        # (1-sum(p*p))*(group_size/instances)\n",
    "        # count all samples at split point\n",
    "        n_instances = float(sum([len(group) for group in groups]))\n",
    "        # sum weighted Gini index for each group\n",
    "        gini = 0.0\n",
    "        for group in groups:\n",
    "            size = float(len(group))\n",
    "            # avoid divide by zero\n",
    "            if size == 0:\n",
    "                continue\n",
    "            score = 0.0\n",
    "            # score the group based on the score for each class\n",
    "            for class_val in classes:\n",
    "                p = [row[-1] for row in group].count(class_val) / size\n",
    "                score += p * p\n",
    "            # weight the group score by its relative size\n",
    "            gini += (1.0 - score) * (size / n_instances)\n",
    "        return gini\n",
    "\n",
    "    # Select the best split point for a dataset\n",
    "    def split_data(self, dataset):\n",
    "        # split input data, and return a node with the split information\n",
    "        class_values = list(set(row[-1] for row in dataset))\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "        for index in range(len(dataset[0])-1):\n",
    "            for row in dataset:\n",
    "                left_child, right_child = test_split(index, row[index], dataset)\n",
    "                gini = self.gini_index((left_child, right_child), class_values)\n",
    "                if gini < b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], gini, left_child, right_child\n",
    "        node = TreeNode(dataset, b_index, b_val, TreeNode(data=left_child), TreeNode(data=right_child))\n",
    "        return node\n",
    "\n",
    "    # Create a terminal node value\n",
    "    def get_terminal_class_val(self, group):\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "    # split a node, get its child information, and class information if it's a terminal node\n",
    "    def split(self, node, max_depth, min_size, depth):\n",
    "        left, right = node['left_child']['data'], node['right_child']['data']\n",
    "        \n",
    "        # check for a no split\n",
    "        if not left or not right or depth >= max_depth:\n",
    "            node['class_value'] = self.get_terminal_class_val(left + right)\n",
    "            node['right_child'] = None\n",
    "            node['left_child'] = None\n",
    "            return\n",
    "        \n",
    "        # process left child\n",
    "        if len(left) <= min_size:\n",
    "            node['left_child']['class_value'] = self.get_terminal_class_val(left)\n",
    "        else:\n",
    "            node['left_child'] = self.split_data(left)\n",
    "            self.split(node['left_child'], max_depth, min_size, depth+1)\n",
    "        \n",
    "        # process right child\n",
    "        if len(right) <= min_size:\n",
    "            node['right_child']['class_value'] = self.get_terminal_class_val(right)\n",
    "        else:\n",
    "            node['right_child'] = self.split_data(right)\n",
    "            self.split(node['right_child'], max_depth, min_size, depth+1)\n",
    " \n",
    "    # Build a decision tree\n",
    "    def build_tree(self, train_data, max_depth, min_size):\n",
    "        # build the tree and return the root of the tree\n",
    "        root = split_data(train_data) \n",
    "        self.split(root, max_depth, min_size, 1)\n",
    "        return root\n",
    "\n",
    "    # Print a decision tree\n",
    "    def print_tree(node, depth=0):\n",
    "        if isinstance(node, dict):\n",
    "            print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "            print_tree(node['left'], depth+1)\n",
    "            print_tree(node['right'], depth+1)\n",
    "        else:\n",
    "            print('%s[%s]' % ((depth*' ', node)))\n",
    "\n",
    "dataset = [[2.771244718,1.784783929,0],\n",
    "\t[1.728571309,1.169761413,0],\n",
    "\t[3.678319846,2.81281357,0],\n",
    "\t[3.961043357,2.61995032,0],\n",
    "\t[2.999208922,2.209014212,0],\n",
    "\t[7.497545867,3.162953546,1],\n",
    "\t[9.00220326,3.339047188,1],\n",
    "\t[7.444542326,0.476683375,1],\n",
    "\t[10.12493903,3.234550982,1],\n",
    "\t[6.642287351,3.319983761,1]]\n",
    "tree = build_tree(dataset, 1, 1)\n",
    "print_tree(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "31b565c6-c83e-46d4-85e7-bcd1553e80f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 1, Got 1.\n"
     ]
    }
   ],
   "source": [
    "# K-nearest neighbor\n",
    "## So the problem becomes how we can calculate the distances between items. The solution \n",
    "## to this depends on the data set. If the values are real we usually use the Euclidean \n",
    "## distance. \n",
    "## If the values are categorical or binary, we usually use the Hamming distance.# count # of different values\n",
    "\n",
    "# Given a new item:\n",
    "#    1. Find distances between new item and all other items\n",
    "#    2. Pick k shorter distances\n",
    "#    3. Pick the most common class in these k distances\n",
    "#    4. That class is where we will classify the new item\n",
    "\n",
    "# Example of making predictions\n",
    "from math import sqrt\n",
    "import heapq\n",
    " \n",
    "\n",
    "class KNearestNeighbor:\n",
    "\n",
    "    \n",
    "    def euclidean_distance(self, row1, row2):\n",
    "        # calculate the Euclidean distance between two vectors\n",
    "        # assume the last element in each row is target variable y\n",
    "        distance = 0.0\n",
    "        for i in range(len(row1)-1):\n",
    "            distance += (row1[i] - row2[i])**2\n",
    "        return sqrt(distance)\n",
    " \n",
    "\n",
    "    def get_neighbors(self, train, test_row, num_neighbors):\n",
    "        # get top K neighbors\n",
    "        distances = list()\n",
    "        for train_row in train:\n",
    "            dist = self.euclidean_distance(test_row, train_row)\n",
    "            distances.append((dist, train_row))\n",
    "        heapq.heapify(distances)\n",
    "        neighbors = []\n",
    "        for i in range(num_neighbors):\n",
    "            dist, row = heapq.heappop(distances)\n",
    "            neighbors.append(row)\n",
    "        return neighbors\n",
    " \n",
    "    def predict_classification(self, train, test_row, num_neighbors):\n",
    "        # Make a classification prediction with neighbors\n",
    "        neighbors = self.get_neighbors(train, test_row, num_neighbors)\n",
    "        output_values = [row[-1] for row in neighbors]\n",
    "        prediction = max(set(output_values), key=output_values.count)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def predict_regression(self, train, test_row, num_neighbors):\n",
    "        # Make a classification prediction with neighbors\n",
    "        neighbors = self.get_neighbors(train, test_row, num_neighbors)\n",
    "        output_values = [row[-1] for row in neighbors]\n",
    "        prediction = np.mean(output_values)\n",
    "        \n",
    "        return prediction\n",
    "# Test distance function\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "knn = KNearestNeighbor()\n",
    "prediction = knn.predict_classification(dataset, dataset[7], 3)\n",
    "print('Expected %d, Got %d.' % (dataset[7][-1], prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5b9bc-6fc6-418b-8317-8f2f5ace5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
    "                                       (self.input_nodes, self.hidden_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.output_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### TODO: Set self.activation_function to your implemented sigmoid function ####\n",
    "        #\n",
    "        # Note: in Python, you can define a function with a lambda expression,\n",
    "        # as shown below.\n",
    "        self.activation_function = lambda x : (1 / (1 + np.exp(-x)))  # Replace 0 with your sigmoid calculation.\n",
    "        \n",
    "        ### If the lambda code above is not something you're familiar with,\n",
    "        # You can uncomment out the following three lines and put your \n",
    "        # implementation there instead.\n",
    "        #\n",
    "        #def sigmoid(x):\n",
    "        #    return 0  # Replace 0 with your sigmoid calculation here\n",
    "        #self.activation_function = sigmoid\n",
    "                    \n",
    "\n",
    "    def train(self, features, targets):\n",
    "        ''' Train the network on batch of features and targets. \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            \n",
    "            features: 2D array, each row is one data record, each column is a feature\n",
    "            targets: 1D array of target values\n",
    "        \n",
    "        '''\n",
    "        n_records = features.shape[0]\n",
    "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n",
    "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n",
    "        for X, y in zip(features, targets):\n",
    "            final_outputs, hidden_outputs = self.forward_pass_train(X)  # Implement the forward pass function below\n",
    "            # Implement the backproagation function below\n",
    "            delta_weights_i_h, delta_weights_h_o = self.backpropagation(final_outputs, hidden_outputs, X, y, \n",
    "                                                                        delta_weights_i_h, delta_weights_h_o)\n",
    "        self.update_weights(delta_weights_i_h, delta_weights_h_o, n_records)\n",
    "\n",
    "\n",
    "    def forward_pass_train(self, X):\n",
    "        ''' Implement forward pass here \n",
    "         \n",
    "            Arguments\n",
    "            ---------\n",
    "            X: features batch\n",
    "        '''\n",
    "        #### Implement the forward pass here ####\n",
    "        ### Forward pass ###\n",
    "        # TODO: Hidden layer - Replace these values with your calculations.\n",
    "        hidden_inputs = np.dot(X, self.weights_input_to_hidden) # signals into hidden layer\n",
    "        hidden_outputs =  self.activation_function(hidden_inputs)# signals from hidden layer\n",
    "\n",
    "\n",
    "        # TODO: Output layer - Replace these values with your calculations.\n",
    "        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
    "        final_outputs = final_inputs # signals from final output layer\n",
    "        \n",
    "        return final_outputs, hidden_outputs\n",
    "\n",
    "    def backpropagation(self, final_outputs, hidden_outputs, X, y, delta_weights_i_h, delta_weights_h_o):\n",
    "        ''' Implement backpropagation\n",
    "         \n",
    "            Arguments\n",
    "            ---------\n",
    "            final_outputs: output from forward pass\n",
    "            y: target (i.e. label) batch\n",
    "            delta_weights_i_h: change in weights from input to hidden layers\n",
    "            delta_weights_h_o: change in weights from hidden to output layers\n",
    "        '''\n",
    "        #### Implement the backward pass here ####\n",
    "        ### Backward pass ###\n",
    "\n",
    "        # TODO: Output error - Replace this value with your calculations.\n",
    "        error = y - final_outputs # Output layer error is the difference between desired target and actual output.\n",
    "         \n",
    "        # TODO: Backpropagated error terms - Replace these values with your calculations.\n",
    "        output_error_term = error \n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = output_error_term.dot(self.weights_hidden_to_output.T) \n",
    "        hidden_error_term = hidden_error *  hidden_outputs * (1 - hidden_outputs)\n",
    "        \n",
    "        # Weight step (input to hidden)\n",
    "        delta_weights_i_h +=  X[:,None] * hidden_error_term \n",
    "        # Weight step (hidden to output)\n",
    "        delta_weights_h_o +=  hidden_outputs[:,None] * output_error_term\n",
    "        return delta_weights_i_h, delta_weights_h_o\n",
    "\n",
    "    def update_weights(self, delta_weights_i_h, delta_weights_h_o, n_records):\n",
    "        ''' Update weights on gradient descent step\n",
    "         \n",
    "            Arguments\n",
    "            ---------\n",
    "            delta_weights_i_h: change in weights from input to hidden layers\n",
    "            delta_weights_h_o: change in weights from hidden to output layers\n",
    "            n_records: number of records\n",
    "        '''\n",
    "        self.weights_hidden_to_output +=self.lr * delta_weights_h_o / n_records\n",
    "        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "    def run(self, features):\n",
    "        ''' Run a forward pass through the network with input features \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            features: 1D array of feature values\n",
    "        '''\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer - replace these values with the appropriate calculations.\n",
    "        hidden_inputs = np.dot(features, self.weights_input_to_hidden) # signals into hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer - Replace these values with the appropriate calculations.\n",
    "        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
    "        final_outputs = final_inputs  # signals from final output layer \n",
    "        \n",
    "        return final_outputs\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# Set your hyperparameters here\n",
    "##########################################################\n",
    "iterations = 4000\n",
    "learning_rate = 0.45\n",
    "hidden_nodes = 30\n",
    "output_nodes = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16f7a81a-942c-4bbc-a107-62bfd99a25e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0 f([-0.14595599  0.42064899]) = 0.19825\n",
      ">1 f([-0.12613855  0.40070573]) = 0.17648\n",
      ">2 f([-0.10665938  0.3808601 ]) = 0.15643\n",
      ">3 f([-0.08770234  0.3611548 ]) = 0.13812\n",
      ">4 f([-0.06947941  0.34163405]) = 0.12154\n",
      ">5 f([-0.05222756  0.32234308]) = 0.10663\n",
      ">6 f([-0.03620086  0.30332769]) = 0.09332\n",
      ">7 f([-0.02165679  0.28463383]) = 0.08149\n",
      ">8 f([-0.00883663  0.26630707]) = 0.07100\n",
      ">9 f([0.00205801 0.24839209]) = 0.06170\n",
      ">10 f([0.01088844 0.23093228]) = 0.05345\n",
      ">11 f([0.01759677 0.2139692 ]) = 0.04609\n",
      ">12 f([0.02221425 0.19754214]) = 0.03952\n",
      ">13 f([0.02485859 0.18168769]) = 0.03363\n",
      ">14 f([0.02572196 0.16643933]) = 0.02836\n",
      ">15 f([0.02505339 0.15182705]) = 0.02368\n",
      ">16 f([0.02313917 0.13787701]) = 0.01955\n",
      ">17 f([0.02028406 0.12461125]) = 0.01594\n",
      ">18 f([0.01679451 0.11204744]) = 0.01284\n",
      ">19 f([0.01296436 0.10019867]) = 0.01021\n",
      ">20 f([0.00906264 0.08907337]) = 0.00802\n",
      ">21 f([0.00532366 0.07867522]) = 0.00622\n",
      ">22 f([0.00193919 0.06900318]) = 0.00477\n",
      ">23 f([-0.00094677  0.06005154]) = 0.00361\n",
      ">24 f([-0.00324034  0.05181012]) = 0.00269\n",
      ">25 f([-0.00489722  0.04426444]) = 0.00198\n",
      ">26 f([-0.00591902  0.03739604]) = 0.00143\n",
      ">27 f([-0.00634719  0.0311828 ]) = 0.00101\n",
      ">28 f([-0.00625503  0.02559933]) = 0.00069\n",
      ">29 f([-0.00573849  0.02061737]) = 0.00046\n",
      ">30 f([-0.00490679  0.01620624]) = 0.00029\n",
      ">31 f([-0.00387317  0.01233332]) = 0.00017\n",
      ">32 f([-0.00274675  0.00896449]) = 0.00009\n",
      ">33 f([-0.00162559  0.00606458]) = 0.00004\n",
      ">34 f([-0.00059149  0.00359785]) = 0.00001\n",
      ">35 f([0.0002934  0.00152838]) = 0.00000\n",
      ">36 f([ 0.00098821 -0.00017954]) = 0.00000\n",
      ">37 f([ 0.00147307 -0.00156101]) = 0.00000\n",
      ">38 f([ 0.00174746 -0.00265025]) = 0.00001\n",
      ">39 f([ 0.00182724 -0.00348028]) = 0.00002\n",
      ">40 f([ 0.00174089 -0.00408267]) = 0.00002\n",
      ">41 f([ 0.00152536 -0.00448737]) = 0.00002\n",
      ">42 f([ 0.00122173 -0.00472254]) = 0.00002\n",
      ">43 f([ 0.00087133 -0.00481438]) = 0.00002\n",
      ">44 f([ 0.00051228 -0.00478712]) = 0.00002\n",
      ">45 f([ 0.00017692 -0.00466292]) = 0.00002\n",
      ">46 f([-0.00011001 -0.00446188]) = 0.00002\n",
      ">47 f([-0.00033219 -0.004202  ]) = 0.00002\n",
      ">48 f([-0.00048176 -0.00389929]) = 0.00002\n",
      ">49 f([-0.00055861 -0.00356777]) = 0.00001\n",
      ">50 f([-0.00056912 -0.00321961]) = 0.00001\n",
      ">51 f([-0.00052452 -0.00286514]) = 0.00001\n",
      ">52 f([-0.00043908 -0.00251304]) = 0.00001\n",
      ">53 f([-0.0003283  -0.00217044]) = 0.00000\n",
      ">54 f([-0.00020731 -0.00184302]) = 0.00000\n",
      ">55 f([-8.95352320e-05 -1.53514076e-03]) = 0.00000\n",
      ">56 f([ 1.43050285e-05 -1.25002847e-03]) = 0.00000\n",
      ">57 f([ 9.67123406e-05 -9.89850279e-04]) = 0.00000\n",
      ">58 f([ 0.00015359 -0.00075587]) = 0.00000\n",
      ">59 f([ 0.00018407 -0.00054858]) = 0.00000\n",
      "Done!\n",
      "f([ 0.00018407 -0.00054858]) = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Adam optimization \n",
    "\n",
    "# gradient descent optimization with adam for a two-dimensional test function\n",
    "from math import sqrt\n",
    "from numpy import asarray\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "\n",
    "f(x) = x*x, here x is a vector, i.e., x = (X1,X2)\n",
    "\n",
    "# objective function\n",
    "def objective(x, y):\n",
    "    return x**2.0 + y**2.0\n",
    " \n",
    "# derivative of objective function\n",
    "def derivative(x, y):\n",
    "    return asarray([x * 2.0, y * 2.0])\n",
    " \n",
    "# gradient descent algorithm with adam\n",
    "def adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2, eps=1e-8):\n",
    "    # generate an initial point\n",
    "    x = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "    score = objective(x[0], x[1])\n",
    "    \n",
    "    # initialize first and second moments\n",
    "    m = [0.0 for _ in range(bounds.shape[0])]\n",
    "    v = [0.0 for _ in range(bounds.shape[0])]\n",
    "    \n",
    "    # run the gradient descent updates\n",
    "    for t in range(n_iter):\n",
    "        \n",
    "        # calculate gradient g(t)\n",
    "        g = derivative(x[0], x[1])\n",
    "        \n",
    "        # build a solution one variable at a time\n",
    "        for i in range(x.shape[0]):\n",
    "            m[i] = beta1 * m[i] + (1.0 - beta1) * g[i]\n",
    "            v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2\n",
    "            mhat = m[i] / (1.0 - beta1**(t+1))\n",
    "            vhat = v[i] / (1.0 - beta2**(t+1))\n",
    "            x[i] = x[i] - alpha * mhat / (sqrt(vhat) + eps)\n",
    "        \n",
    "        # evaluate candidate point\n",
    "        score = objective(x[0], x[1])\n",
    "        # report progress\n",
    "        print('>%d f(%s) = %.5f' % (t, x, score))\n",
    "    return [x, score]\n",
    " \n",
    "# seed the pseudo random number generator\n",
    "seed(1)\n",
    "# define range for input [[X1_min,X1_max],[X2_min, X2_max]]\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 60\n",
    "# steps size\n",
    "alpha = 0.02\n",
    "# factor for average gradient\n",
    "beta1 = 0.8\n",
    "# factor for average squared gradient\n",
    "beta2 = 0.999\n",
    "# perform the gradient descent search with adam\n",
    "best, score = adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608402d1-f691-4106-b0c0-9000092d4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP \n",
    "# bigram\n",
    "# tf-idf (total frequency -inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076457f2-52ba-4ae5-844f-9d43679b52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c\n",
    "# simple random sampling\n",
    "sample_df = df.sample(100)\n",
    "\n",
    "# Stratified sampling\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
