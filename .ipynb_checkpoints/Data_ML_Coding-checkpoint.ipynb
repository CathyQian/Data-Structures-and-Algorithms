{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# K-means clustering\r\n",
    "# K-nearest neighbor\r\n",
    "# PCA\r\n",
    "# linear regression\r\n",
    "# logistic regression\r\n",
    "# SVM\r\n",
    "# decision tree\r\n",
    "# Sampling\r\n",
    "## stratified sampling\r\n",
    "## uniform sampling\r\n",
    "## reservoir sampling\r\n",
    "## sampling multinomial distribution\r\n",
    "## random generator\r\n",
    "# NLP algorithms (if that's your area of work)\r\n",
    "## bigrams\r\n",
    "## tf-idf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# K-means clustering\r\n",
    "import numpy as np\r\n",
    "def kMeansClustering(X, K):\r\n",
    "    Cx = np.random.random_sample(K)*(np.amax(X)- np.amin(X)) + np.amin(X)\r\n",
    "    Cy = np.random.random_sample(K)*(np.amax(X)- np.amin(X)) + np.amin(X)\r\n",
    "    C = np.array(list(zip(Cx, Cy)), dtype=np.float32) # numpy array is faster than python list\r\n",
    "    print(np.shape(C))\r\n",
    "\r\n",
    "    for i in range(100): # max 100 iterations\r\n",
    "        m = len(X)\r\n",
    "        idx = np.zeros(m)\r\n",
    "        for i in range(m):\r\n",
    "            dist = np.linalg.norm(X[i] - C, axis = 1)\r\n",
    "            idx[i] = np.argmin(dist)\r\n",
    "\r\n",
    "        # update the position of the K centroids\r\n",
    "        for i in range(K):\r\n",
    "            points = [X[j] for j in range(m) if idx[j] == i]\r\n",
    "            C[i] = np.mean(points, axis = 0)\r\n",
    "    return C\r\n",
    "\r\n",
    "arrays = [[12, 39], [20, 36], [28, 30], [18, 52], [29, 54], [33, 46], [24, 55],\r\n",
    "[45, 59], [45, 63], [52, 70], [51, 66], [52, 63], [55, 58], [53, 23], [55, 14],\r\n",
    "[61, 8], [64, 19], [69, 7], [72, 24]]\r\n",
    "C = kMeansClustering(arrays,3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "x = [x[0] for x in arrays]\r\n",
    "y = [x[1] for x in arrays]\r\n",
    "Cx = [x[0] for x in C]\r\n",
    "Cy = [x[1] for x in C]\r\n",
    "plt.scatter(x, y, color='g')\r\n",
    "plt.scatter(Cx, Cy, color='r')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2a77a5dba88>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# PCA\r\n",
    "## Scale the data — we don’t want some feature to be voted as “more important” due to scale differences.\r\n",
    "## Calculate covariance matrix — square matrix giving the covariances between each pair of elements of a random vector\r\n",
    "## Eigendecomposition\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "Cx = np.random.random_sample(30)*4 + 10\r\n",
    "Cy = np.random.random_sample(30)*40 + 10\r\n",
    "Cz = np.random.random_sample(30)*100 + 10\r\n",
    "X = np.array(list(zip(Cx, Cy, Cz)), dtype=np.float32) # numpy array is faster than python list\r\n",
    "    \r\n",
    "X_scaled = StandardScaler().fit_transform(X) #30x3\r\n",
    "features = X_scaled.T # 3x30\r\n",
    "cov_matrix = np.cov(features) # 3x3\r\n",
    "values, vectors = np.linalg.eig(cov_matrix) # eigen decomposition, results ordered by variance of vectors(3x3)\r\n",
    "\r\n",
    "explained_variances = []\r\n",
    "for i in range(len(values)):\r\n",
    "    explained_variances.append(values[i]/np.sum(values))\r\n",
    "\r\n",
    "PC1 = np.dot(X_scaled, vectors.T[0]) # 30x3, 3x1  ---> 30x1\r\n",
    "PC2 = np.dot(X_scaled, vectors.T[1])\r\n",
    "sns.scatterplot(PC1, PC2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zqian\\AppData\\Roaming\\Python\\Python37\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "np.shape(PC1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "vectors.T[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.46027938,  0.60291617,  0.65164023])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# linear regression\r\n",
    "# least square coefficient estimate\r\n",
    "\r\n",
    "class LinearRegression:\r\n",
    "    def __init__(self):\r\n",
    "        self.m = 0\r\n",
    "        self.b = 0\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.m = sum((X-np.mean(X))*(y-np.mean(y)))/sum((X-np.mean(X))**2)\r\n",
    "        self.b = np.mean(y) - self.m*np.mean(X)\r\n",
    "    def predict(self, X): \r\n",
    "        return self.m*X + self.b\r\n",
    "    def coef(self, y, y_hat):\r\n",
    "        return 1 - sum((y-y_hat)**2)/sum((y-np.mean(y))**2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "X = np.linspace(0,10,10)\r\n",
    "m, b  = 3, -2\r\n",
    "Y = m * X + b + 0.1 * np.random.randn(X.shape[0])\r\n",
    "\r\n",
    "lr = LinearRegression()\r\n",
    "lr.fit(X,Y)\r\n",
    "Y_hat = lr.predict(X)\r\n",
    "R2 = lr.coef(Y,Y_hat)\r\n",
    "\r\n",
    "print(lr.m, lr.b)\r\n",
    "print(R2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.992412056956919 -1.911093353240874\n",
      "0.9999626705987037\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# Logistic Regression\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, learning_rate, max_iteration):\n",
    "        self.theta = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iteration = max_iteration\n",
    "    \n",
    "    # method\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1+np.exp(-z))\n",
    "    \n",
    "    def cost_func(self, X, y): # both h and y are 1D array\n",
    "        h = self.sigmoid(np.dot(X, self.theta))\n",
    "        J = 1.0/m *np.sum(-y*np.log(h) - (1.0-y)*np.log(1.0-h))\n",
    "        return J\n",
    "    \n",
    "    def gradients(self, X, y):\n",
    "        h = self.sigmoid(np.dot(X, self.theta))\n",
    "        return 1.0/m * np.dot(X.T, (h-y))\n",
    "    \n",
    "    def fit(self, X, y): # X is mxn dimension, m=sample size, n=feature dimension\n",
    "        cost = []\n",
    "        X = np.array([[1] + x for x in X]) # add x0=1 to all X\n",
    "        self.theta = np.random.rand(X.shape[1]) # array with n elements\n",
    "        for it in range(self.max_iteration):\n",
    "            cost.append(self.cost_func(X, y))\n",
    "            grads = self.gradients(X, y)\n",
    "            self.theta -= self.learning_rate*grads\n",
    "        print(cost)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        pred = self.sigmoid(np.dot(X, self.theta))\n",
    "        return pred\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "x1 = np.random.randn(5,3) + 5\n",
    "x2 = np.random.randn(5,3) - 5\n",
    "X = np.concatenate([x1,x2], axis=0)    \n",
    "y  = np.concatenate([np.ones(5), -np.zeros(5)], axis=0)   \n",
    "lr = LogisticRegression(learning_rate=0.01, max_iteration=100)\n",
    "lr.fit(X,y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.049035790518848515, 0.048117772958766125, 0.047233481019227794, 0.04638109384600978, 0.04555891907947929, 0.04476538173582384, 0.043999014221549865, 0.043258447348895335, 0.04254240223717429, 0.04184968299989751, 0.04117917013024033, 0.04052981450835952, 0.0399006319634831, 0.03929069833183019, 0.038699144958461974, 0.03812515459727937, 0.037567957668688395, 0.03702682883909614, 0.0365010838904334, 0.03599007685144568, 0.035493197365588314, 0.03500986827309392, 0.034539543387168456, 0.03408170544639312, 0.03363586422726836, 0.03320155480249186, 0.032778335932017996, 0.03236578857525193, 0.031963514513878265, 0.03157113507584996, 0.031188289951983826, 0.030814636097421095, 0.030449846710941537, 0.030093610285776007, 0.029745629726146236, 0.029405621524286503, 0.029073314993175337, 0.028748451550629925, 0.02843078405079887, 0.028120076159436604, 0.02781610176965015, 0.027518644455097236, 0.027227496957864573, 0.026942460708489633, 0.026663345375799168, 0.026389968444425056, 0.026122154818036036, 0.025859736446477405, 0.025602551975157224, 0.02535044641514338, 0.025103270832565194, 0.024860882056007894, 0.024623142400703346, 0.024389919408398054, 0.02416108560187322, 0.023936518253161305, 0.023716099164576315, 0.023499714461741705, 0.02328725439785369, 0.023078613168475745, 0.02287368873621193, 0.022672382664647165, 0.02247459996098896, 0.022280248926884447, 0.02208924101692271, 0.02190149070436074, 0.02171691535365137, 0.021535435099372016, 0.021356972731182547, 0.02118145358446722, 0.02100880543633244, 0.02083895840666067, 0.02067184486393402, 0.020507399335561387, 0.020345558422461045, 0.020186260717664866, 0.020029446728724094, 0.019875058803712656, 0.019723041060634307, 0.019573339320052056, 0.019425901040770497, 0.019280675258409572, 0.019137612526721852, 0.01899666486150814, 0.01885778568700131, 0.01872092978459034, 0.018586053243767806, 0.018453113415188252, 0.018322068865732996, 0.018192879335481475, 0.018065505696495857, 0.01793990991332986, 0.017816055005179375, 0.017693905009594703, 0.017573424947679973, 0.017454580790709877, 0.017337339428095927, 0.017221668636640028, 0.01710753705101405, 0.016994914135410974]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "lr.predict_proba(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([9.99691285e-01, 9.99766336e-01, 9.99551114e-01, 9.99710488e-01,\n",
       "       9.99443771e-01, 1.03324231e-03, 5.73465980e-04, 1.23758232e-03,\n",
       "       5.73662186e-03, 2.50268587e-03])"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# SVM\r\n",
    "# https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\r\n",
    "\r\n",
    "\r\n",
    "class SupportVectorMachine:\r\n",
    "    # constructor\r\n",
    "    def __init__(self, C, learning_rate, max_iteration):\r\n",
    "        self.weights = None\r\n",
    "        self.C = C\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.max_iteration = max_iteration\r\n",
    "        \r\n",
    "    def cost_func(self, W, X, y):\r\n",
    "        # compute the hinge loss\r\n",
    "        N = X.shape[0]\r\n",
    "        distances = 1 - y*(np.dot(X, W))\r\n",
    "        distances[distances<0] = 0 # equivalent to max(0, distance)\r\n",
    "        hinge_loss = self.C * (np.sum(distances)/N)\r\n",
    "        cost = 1/2*np.dot(W[1:], W[1:]) + hinge_loss\r\n",
    "        return cost\r\n",
    "\r\n",
    "    def gradients(self, W, X, y):\r\n",
    "        distances = 1 - y*np.dot(X, W)\r\n",
    "        gradients = np.zeros(len(W))\r\n",
    "        for idx, d in enumerate(distances):\r\n",
    "            if max(0, d) == 0:\r\n",
    "                gradients += W\r\n",
    "            else:\r\n",
    "                gradients += W - self.C * y[idx] * X[idx]\r\n",
    "        return gradients\r\n",
    "    \r\n",
    "    def fit(self, X, y): # X is mxn dimension, m=sample size, n=feature dimension\r\n",
    "        cost = []\r\n",
    "        X = np.array([[1] + x for x in X]) # add x0=1 to all X\r\n",
    "        y = [1 if y_ == 1 else -1 for y_ in y ]\r\n",
    "        self.weights = np.random.rand(X.shape[1]) # array with n elements\r\n",
    "        for it in range(self.max_iteration):\r\n",
    "            cost.append(self.cost_func(self.weights, X, y))\r\n",
    "            grads = self.gradients(self.weights, X, y)\r\n",
    "            self.weights -= self.learning_rate*grads\r\n",
    "        print(cost)\r\n",
    "        \r\n",
    "    def predict(self, X):\r\n",
    "        f_x = np.dot(X, self.weights)\r\n",
    "        pred = [1 if f_x_ >= 0 else 0 for f_x_ in f_x]\r\n",
    "        return pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x1 = np.random.randn(5,3) + 5\r\n",
    "x2 = np.random.randn(5,3) - 5\r\n",
    "X = np.concatenate([x1,x2], axis=0)    \r\n",
    "y  = np.concatenate([np.ones(5), -np.zeros(5)], axis=0)   \r\n",
    "svm = SupportVectorMachine(C=1, learning_rate=0.01,max_iteration=100)\r\n",
    "svm.fit(X,y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.6881522945615793, 0.5574033585948792, 0.4514967204618522, 0.36571234357410026, 0.2962269982950212, 0.23994386861896716, 0.19435453358136343, 0.15742717220090435, 0.1275160094827325, 0.10328796768101334, 0.0836632538216208, 0.06776723559551286, 0.05489146083236541, 0.04446208327421598, 0.03601428745211495, 0.029171572836213105, 0.02362897399733261, 0.019139468937839417, 0.01550296983964993, 0.012557405570116443, 0.010769535767618032, 0.015227950252609767, 0.01233463970461391, 0.009991058160737268, 0.011801104758421874, 0.02393004457519746, 0.01938333610590994, 0.015700502245787052, 0.012717406819087512, 0.010301099523460883, 0.008343890614003315, 0.029573257311777693, 0.034359152669540004, 0.0278309136623274, 0.022543040066485196, 0.01825986245385301, 0.014790488587620937, 0.01198029575597296, 0.009704039562338097, 0.008317761755388545, 0.01455656926680602, 0.011790821106112877, 0.00955056509595143, 0.01645335980496865, 0.036328556108319275, 0.029426130447738615, 0.02383516566266828, 0.019306484186761307, 0.01563825219127666, 0.012666984274934092, 0.010260257262696615, 0.008310808382784259, 0.027950939999202278, 0.034331048158700805, 0.02780814900854765, 0.022524600696923597, 0.018244926564508112, 0.01477839051725157, 0.011970496318973772, 0.009696102018368754, 0.00800492643365421, 0.014562381999033957, 0.011795529419217505, 0.009554378829566178, 0.016005826755956278, 0.036329250959208706, 0.029426693276959053, 0.02383562155433683, 0.01930685345901283, 0.01563855130180039, 0.012667226554458316, 0.010260453509111237, 0.008310967342380102, 0.027776488493763924, 0.03433045225164763, 0.02780766632383458, 0.022524209722306014, 0.018244609875067867, 0.014778133998804973, 0.011970288539032028, 0.009695933716615942, 0.007971263484175857, 0.014563477434222694, 0.011796416721720382, 0.00955509754459351, 0.01595710229245222, 0.036329524750851286, 0.029426915048189544, 0.02383580118903353, 0.019306998963117158, 0.0156386691601249, 0.012667322019701168, 0.010260530835957946, 0.008311029977125937, 0.027757436832039833, 0.0343304167310796, 0.027807637552174476, 0.022524186417261324, 0.018244590997981674, 0.014778118708365153]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "svm.predict(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "# decision tree\r\n",
    "\r\n",
    "class DecisionTree:\r\n",
    "    \r\n",
    "    def TreeNode(self, data = data, feature_index = None, split_val=None, left_child=None, right_child=None, class_value=None): # a treenode is defined by its split condition\r\n",
    "        self.data = data\r\n",
    "        self.feature_index = feature_index # feature index\r\n",
    "        self.split_val = split_val # only consider numerical val\r\n",
    "        self.left_child = left_child # TreeNode\r\n",
    "        self.right_child = right_child # TreeNode\r\n",
    "        self.class_value = class_value # only exist for terminal node\r\n",
    "        \r\n",
    "    # Split a dataset based on an attribute and an attribute value\r\n",
    "    def test_split(self, feature_index, value, dataset):\r\n",
    "        left_child, right_child = list(), list()\r\n",
    "        for row in dataset:\r\n",
    "            if row[feature_index] < value:\r\n",
    "                left.append(row)\r\n",
    "            else:\r\n",
    "                right.append(row)\r\n",
    "        return left_child, right_child\r\n",
    " \r\n",
    "    # Calculate the Gini index for a split dataset\r\n",
    "    def gini_index(self, groups, classes):\r\n",
    "        # (1-sum(p*p))*(group_size/instances)\r\n",
    "        # count all samples at split point\r\n",
    "        n_instances = float(sum([len(group) for group in groups]))\r\n",
    "        # sum weighted Gini index for each group\r\n",
    "        gini = 0.0\r\n",
    "        for group in groups:\r\n",
    "            size = float(len(group))\r\n",
    "            # avoid divide by zero\r\n",
    "            if size == 0:\r\n",
    "                continue\r\n",
    "            score = 0.0\r\n",
    "            # score the group based on the score for each class\r\n",
    "            for class_val in classes:\r\n",
    "                p = [row[-1] for row in group].count(class_val) / size\r\n",
    "                score += p * p\r\n",
    "            # weight the group score by its relative size\r\n",
    "            gini += (1.0 - score) * (size / n_instances)\r\n",
    "        return gini\r\n",
    "\r\n",
    "    # Select the best split point for a dataset\r\n",
    "    def split_data(self, dataset):\r\n",
    "        # split input data, and return a node with the split information\r\n",
    "        class_values = list(set(row[-1] for row in dataset))\r\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\r\n",
    "        for index in range(len(dataset[0])-1):\r\n",
    "            for row in dataset:\r\n",
    "                left_child, right_child = test_split(index, row[index], dataset)\r\n",
    "                gini = self.gini_index((left_child, right_child), class_values)\r\n",
    "                if gini < b_score:\r\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], gini, left_child, right_child\r\n",
    "        node = TreeNode(dataset, b_index, b_val, TreeNode(data=left_child), TreeNode(data=right_child))\r\n",
    "        return node\r\n",
    "\r\n",
    "    # Create a terminal node value\r\n",
    "    def get_terminal_class_val(self, group):\r\n",
    "        outcomes = [row[-1] for row in group]\r\n",
    "        return max(set(outcomes), key=outcomes.count)\r\n",
    "\r\n",
    "    # split a node, get its child information, and class information if it's a terminal node\r\n",
    "    def split(self, node, max_depth, min_size, depth):\r\n",
    "        left, right = node['left_child']['data'], node['right_child']['data']\r\n",
    "        \r\n",
    "        # check for a no split\r\n",
    "        if not left or not right or depth >= max_depth:\r\n",
    "            node['class_value'] = self.get_terminal_class_val(left + right)\r\n",
    "            node['right_child'] = None\r\n",
    "            node['left_child'] = None\r\n",
    "            return\r\n",
    "        \r\n",
    "        # process left child\r\n",
    "        if len(left) <= min_size:\r\n",
    "            node['left_child']['class_value'] = self.get_terminal_class_val(left)\r\n",
    "        else:\r\n",
    "            node['left_child'] = self.split_data(left)\r\n",
    "            self.split(node['left_child'], max_depth, min_size, depth+1)\r\n",
    "        \r\n",
    "        # process right child\r\n",
    "        if len(right) <= min_size:\r\n",
    "            node['right_child']['class_value'] = self.get_terminal_class_val(right)\r\n",
    "        else:\r\n",
    "            node['right_child'] = self.split_data(right)\r\n",
    "            self.split(node['right_child'], max_depth, min_size, depth+1)\r\n",
    " \r\n",
    "    # Build a decision tree\r\n",
    "    def build_tree(self, train_data, max_depth, min_size):\r\n",
    "        # build the tree and return the root of the tree\r\n",
    "        root = split_data(train_data) \r\n",
    "        self.split(root, max_depth, min_size, 1)\r\n",
    "        return root\r\n",
    "\r\n",
    "    # Print a decision tree\r\n",
    "    def print_tree(node, depth=0):\r\n",
    "        if isinstance(node, dict):\r\n",
    "            print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\r\n",
    "            print_tree(node['left'], depth+1)\r\n",
    "            print_tree(node['right'], depth+1)\r\n",
    "        else:\r\n",
    "            print('%s[%s]' % ((depth*' ', node)))\r\n",
    "\r\n",
    "dataset = [[2.771244718,1.784783929,0],\r\n",
    "\t[1.728571309,1.169761413,0],\r\n",
    "\t[3.678319846,2.81281357,0],\r\n",
    "\t[3.961043357,2.61995032,0],\r\n",
    "\t[2.999208922,2.209014212,0],\r\n",
    "\t[7.497545867,3.162953546,1],\r\n",
    "\t[9.00220326,3.339047188,1],\r\n",
    "\t[7.444542326,0.476683375,1],\r\n",
    "\t[10.12493903,3.234550982,1],\r\n",
    "\t[6.642287351,3.319983761,1]]\r\n",
    "tree = build_tree(dataset, 1, 1)\r\n",
    "print_tree(tree)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[X1 < 6.642]\n",
      " [0]\n",
      " [1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# K-nearest neighbor\r\n",
    "## So the problem becomes how we can calculate the distances between items. The solution \r\n",
    "## to this depends on the data set. If the values are real we usually use the Euclidean \r\n",
    "## distance. \r\n",
    "## If the values are categorical or binary, we usually use the Hamming distance.# count # of different values\r\n",
    "\r\n",
    "# Given a new item:\r\n",
    "#    1. Find distances between new item and all other items\r\n",
    "#    2. Pick k shorter distances\r\n",
    "#    3. Pick the most common class in these k distances\r\n",
    "#    4. That class is where we will classify the new item\r\n",
    "\r\n",
    "# Example of making predictions\r\n",
    "from math import sqrt\r\n",
    "import heapq\r\n",
    " \r\n",
    "\r\n",
    "class KNearestNeighbor:\r\n",
    "\r\n",
    "    \r\n",
    "    def euclidean_distance(self, row1, row2):\r\n",
    "        # calculate the Euclidean distance between two vectors\r\n",
    "        # assume the last element in each row is target variable y\r\n",
    "        distance = 0.0\r\n",
    "        for i in range(len(row1)-1):\r\n",
    "            distance += (row1[i] - row2[i])**2\r\n",
    "        return sqrt(distance)\r\n",
    " \r\n",
    "\r\n",
    "    def get_neighbors(self, train, test_row, num_neighbors):\r\n",
    "        # get top K neighbors\r\n",
    "        distances = list()\r\n",
    "        for train_row in train:\r\n",
    "            dist = self.euclidean_distance(test_row, train_row)\r\n",
    "            distances.append((dist, train_row))\r\n",
    "        heapq.heapify(distances)\r\n",
    "        neighbors = []\r\n",
    "        for i in range(num_neighbors):\r\n",
    "            dist, row = heapq.heappop(distances)\r\n",
    "            neighbors.append(row)\r\n",
    "        return neighbors\r\n",
    " \r\n",
    "    def predict_classification(self, train, test_row, num_neighbors):\r\n",
    "        # Make a classification prediction with neighbors\r\n",
    "        neighbors = self.get_neighbors(train, test_row, num_neighbors)\r\n",
    "        output_values = [row[-1] for row in neighbors]\r\n",
    "        prediction = max(set(output_values), key=output_values.count)\r\n",
    "        \r\n",
    "        return prediction\r\n",
    "    \r\n",
    "    def predict_regression(self, train, test_row, num_neighbors):\r\n",
    "        # Make a classification prediction with neighbors\r\n",
    "        neighbors = self.get_neighbors(train, test_row, num_neighbors)\r\n",
    "        output_values = [row[-1] for row in neighbors]\r\n",
    "        prediction = np.mean(output_values)\r\n",
    "        \r\n",
    "        return prediction\r\n",
    "# Test distance function\r\n",
    "dataset = [[2.7810836,2.550537003,0],\r\n",
    "\t[1.465489372,2.362125076,0],\r\n",
    "\t[3.396561688,4.400293529,0],\r\n",
    "\t[1.38807019,1.850220317,0],\r\n",
    "\t[3.06407232,3.005305973,0],\r\n",
    "\t[7.627531214,2.759262235,1],\r\n",
    "\t[5.332441248,2.088626775,1],\r\n",
    "\t[6.922596716,1.77106367,1],\r\n",
    "\t[8.675418651,-0.242068655,1],\r\n",
    "\t[7.673756466,3.508563011,1]]\r\n",
    "knn = KNearestNeighbor()\r\n",
    "prediction = knn.predict_classification(dataset, dataset[7], 3)\r\n",
    "print('Expected %d, Got %d.' % (dataset[7][-1], prediction))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Expected 1, Got 1.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# neural network\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "class NeuralNetwork:\r\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\r\n",
    "        # Set number of nodes in input, hidden and output layers.\r\n",
    "        self.input_nodes = input_nodes\r\n",
    "        self.hidden_nodes = hidden_nodes\r\n",
    "        self.output_nodes = output_nodes\r\n",
    "\r\n",
    "        # Initialize weights\r\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \r\n",
    "                                       (self.input_nodes, self.hidden_nodes))\r\n",
    "\r\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \r\n",
    "                                       (self.hidden_nodes, self.output_nodes))\r\n",
    "        self.lr = learning_rate\r\n",
    "        \r\n",
    "        #### TODO: Set self.activation_function to your implemented sigmoid function ####\r\n",
    "        #\r\n",
    "        # Note: in Python, you can define a function with a lambda expression,\r\n",
    "        # as shown below.\r\n",
    "        self.activation_function = lambda x : (1 / (1 + np.exp(-x)))  # Replace 0 with your sigmoid calculation.\r\n",
    "        \r\n",
    "        ### If the lambda code above is not something you're familiar with,\r\n",
    "        # You can uncomment out the following three lines and put your \r\n",
    "        # implementation there instead.\r\n",
    "        #\r\n",
    "        #def sigmoid(x):\r\n",
    "        #    return 0  # Replace 0 with your sigmoid calculation here\r\n",
    "        #self.activation_function = sigmoid\r\n",
    "                    \r\n",
    "\r\n",
    "    def train(self, features, targets):\r\n",
    "        ''' Train the network on batch of features and targets. \r\n",
    "        \r\n",
    "            Arguments\r\n",
    "            ---------\r\n",
    "            \r\n",
    "            features: 2D array, each row is one data record, each column is a feature\r\n",
    "            targets: 1D array of target values\r\n",
    "        \r\n",
    "        '''\r\n",
    "        n_records = features.shape[0]\r\n",
    "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\r\n",
    "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\r\n",
    "        for X, y in zip(features, targets):\r\n",
    "            final_outputs, hidden_outputs = self.forward_pass_train(X)  # Implement the forward pass function below\r\n",
    "            # Implement the backproagation function below\r\n",
    "            delta_weights_i_h, delta_weights_h_o = self.backpropagation(final_outputs, hidden_outputs, X, y, \r\n",
    "                                                                        delta_weights_i_h, delta_weights_h_o)\r\n",
    "        self.update_weights(delta_weights_i_h, delta_weights_h_o, n_records)\r\n",
    "\r\n",
    "\r\n",
    "    def forward_pass_train(self, X):\r\n",
    "        ''' Implement forward pass here \r\n",
    "         \r\n",
    "            Arguments\r\n",
    "            ---------\r\n",
    "            X: features batch\r\n",
    "        '''\r\n",
    "        #### Implement the forward pass here ####\r\n",
    "        ### Forward pass ###\r\n",
    "        # TODO: Hidden layer - Replace these values with your calculations.\r\n",
    "        hidden_inputs = np.dot(X, self.weights_input_to_hidden) # signals into hidden layer\r\n",
    "        hidden_outputs =  self.activation_function(hidden_inputs)# signals from hidden layer\r\n",
    "\r\n",
    "\r\n",
    "        # TODO: Output layer - Replace these values with your calculations.\r\n",
    "        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\r\n",
    "        final_outputs = final_inputs # signals from final output layer\r\n",
    "        \r\n",
    "        return final_outputs, hidden_outputs\r\n",
    "\r\n",
    "    def backpropagation(self, final_outputs, hidden_outputs, X, y, delta_weights_i_h, delta_weights_h_o):\r\n",
    "        ''' Implement backpropagation\r\n",
    "         \r\n",
    "            Arguments\r\n",
    "            ---------\r\n",
    "            final_outputs: output from forward pass\r\n",
    "            y: target (i.e. label) batch\r\n",
    "            delta_weights_i_h: change in weights from input to hidden layers\r\n",
    "            delta_weights_h_o: change in weights from hidden to output layers\r\n",
    "        '''\r\n",
    "        #### Implement the backward pass here ####\r\n",
    "        ### Backward pass ###\r\n",
    "\r\n",
    "        # TODO: Output error - Replace this value with your calculations.\r\n",
    "        error = y - final_outputs # Output layer error is the difference between desired target and actual output.\r\n",
    "         \r\n",
    "        # TODO: Backpropagated error terms - Replace these values with your calculations.\r\n",
    "        output_error_term = error \r\n",
    "\r\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\r\n",
    "        hidden_error = output_error_term.dot(self.weights_hidden_to_output.T) \r\n",
    "        hidden_error_term = hidden_error *  hidden_outputs * (1 - hidden_outputs)\r\n",
    "        \r\n",
    "        # Weight step (input to hidden)\r\n",
    "        delta_weights_i_h +=  X[:,None] * hidden_error_term \r\n",
    "        # Weight step (hidden to output)\r\n",
    "        delta_weights_h_o +=  hidden_outputs[:,None] * output_error_term\r\n",
    "        return delta_weights_i_h, delta_weights_h_o\r\n",
    "\r\n",
    "    def update_weights(self, delta_weights_i_h, delta_weights_h_o, n_records):\r\n",
    "        ''' Update weights on gradient descent step\r\n",
    "         \r\n",
    "            Arguments\r\n",
    "            ---------\r\n",
    "            delta_weights_i_h: change in weights from input to hidden layers\r\n",
    "            delta_weights_h_o: change in weights from hidden to output layers\r\n",
    "            n_records: number of records\r\n",
    "        '''\r\n",
    "        self.weights_hidden_to_output +=self.lr * delta_weights_h_o / n_records\r\n",
    "        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records # update input-to-hidden weights with gradient descent step\r\n",
    "\r\n",
    "    def run(self, features):\r\n",
    "        ''' Run a forward pass through the network with input features \r\n",
    "        \r\n",
    "            Arguments\r\n",
    "            ---------\r\n",
    "            features: 1D array of feature values\r\n",
    "        '''\r\n",
    "        \r\n",
    "        #### Implement the forward pass here ####\r\n",
    "        # TODO: Hidden layer - replace these values with the appropriate calculations.\r\n",
    "        hidden_inputs = np.dot(features, self.weights_input_to_hidden) # signals into hidden layer\r\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\r\n",
    "        \r\n",
    "        # TODO: Output layer - Replace these values with the appropriate calculations.\r\n",
    "        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\r\n",
    "        final_outputs = final_inputs  # signals from final output layer \r\n",
    "        \r\n",
    "        return final_outputs\r\n",
    "\r\n",
    "\r\n",
    "#########################################################\r\n",
    "# Set your hyperparameters here\r\n",
    "##########################################################\r\n",
    "iterations = 4000\r\n",
    "learning_rate = 0.45\r\n",
    "hidden_nodes = 30\r\n",
    "output_nodes = 1\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# Adam optimization \r\n",
    "\r\n",
    "# gradient descent optimization with adam for a two-dimensional test function\r\n",
    "from math import sqrt\r\n",
    "from numpy import asarray\r\n",
    "from numpy.random import rand\r\n",
    "from numpy.random import seed\r\n",
    "\r\n",
    "f(x) = x*x, here x is a vector, i.e., x = (X1,X2)\r\n",
    "\r\n",
    "# objective function\r\n",
    "def objective(x, y):\r\n",
    "    return x**2.0 + y**2.0\r\n",
    " \r\n",
    "# derivative of objective function\r\n",
    "def derivative(x, y):\r\n",
    "    return asarray([x * 2.0, y * 2.0])\r\n",
    " \r\n",
    "# adam optimization\r\n",
    "def adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2, eps=1e-8):\r\n",
    "    # generate an initial point\r\n",
    "    x = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\r\n",
    "    score = objective(x[0], x[1])\r\n",
    "    \r\n",
    "    # initialize first and second moments\r\n",
    "    m = [0.0 for _ in range(bounds.shape[0])]\r\n",
    "    v = [0.0 for _ in range(bounds.shape[0])]\r\n",
    "    \r\n",
    "    # run the gradient descent updates\r\n",
    "    for t in range(n_iter):\r\n",
    "        \r\n",
    "        # calculate gradient g(t)\r\n",
    "        g = derivative(x[0], x[1])\r\n",
    "        \r\n",
    "        # build a solution one variable at a time\r\n",
    "        for i in range(x.shape[0]):\r\n",
    "            m[i] = beta1 * m[i] + (1.0 - beta1) * g[i]\r\n",
    "            v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2\r\n",
    "            mhat = m[i] / (1.0 - beta1**(t+1))\r\n",
    "            vhat = v[i] / (1.0 - beta2**(t+1))\r\n",
    "            x[i] = x[i] - alpha * mhat / (sqrt(vhat) + eps)\r\n",
    "        \r\n",
    "        # evaluate candidate point\r\n",
    "        score = objective(x[0], x[1])\r\n",
    "        # report progress\r\n",
    "        print('>%d f(%s) = %.5f' % (t, x, score))\r\n",
    "    return [x, score]\r\n",
    " \r\n",
    "# seed the pseudo random number generator\r\n",
    "seed(1)\r\n",
    "# define range for input [[X1_min,X1_max],[X2_min, X2_max]]\r\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\r\n",
    "# define the total iterations\r\n",
    "n_iter = 60\r\n",
    "# steps size\r\n",
    "alpha = 0.02\r\n",
    "# factor for average gradient\r\n",
    "beta1 = 0.8\r\n",
    "# factor for average squared gradient\r\n",
    "beta2 = 0.999\r\n",
    "# perform the gradient descent search with adam\r\n",
    "best, score = adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2)\r\n",
    "print('Done!')\r\n",
    "print('f(%s) = %f' % (best, score))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">0 f([-0.14595599  0.42064899]) = 0.19825\n",
      ">1 f([-0.12613855  0.40070573]) = 0.17648\n",
      ">2 f([-0.10665938  0.3808601 ]) = 0.15643\n",
      ">3 f([-0.08770234  0.3611548 ]) = 0.13812\n",
      ">4 f([-0.06947941  0.34163405]) = 0.12154\n",
      ">5 f([-0.05222756  0.32234308]) = 0.10663\n",
      ">6 f([-0.03620086  0.30332769]) = 0.09332\n",
      ">7 f([-0.02165679  0.28463383]) = 0.08149\n",
      ">8 f([-0.00883663  0.26630707]) = 0.07100\n",
      ">9 f([0.00205801 0.24839209]) = 0.06170\n",
      ">10 f([0.01088844 0.23093228]) = 0.05345\n",
      ">11 f([0.01759677 0.2139692 ]) = 0.04609\n",
      ">12 f([0.02221425 0.19754214]) = 0.03952\n",
      ">13 f([0.02485859 0.18168769]) = 0.03363\n",
      ">14 f([0.02572196 0.16643933]) = 0.02836\n",
      ">15 f([0.02505339 0.15182705]) = 0.02368\n",
      ">16 f([0.02313917 0.13787701]) = 0.01955\n",
      ">17 f([0.02028406 0.12461125]) = 0.01594\n",
      ">18 f([0.01679451 0.11204744]) = 0.01284\n",
      ">19 f([0.01296436 0.10019867]) = 0.01021\n",
      ">20 f([0.00906264 0.08907337]) = 0.00802\n",
      ">21 f([0.00532366 0.07867522]) = 0.00622\n",
      ">22 f([0.00193919 0.06900318]) = 0.00477\n",
      ">23 f([-0.00094677  0.06005154]) = 0.00361\n",
      ">24 f([-0.00324034  0.05181012]) = 0.00269\n",
      ">25 f([-0.00489722  0.04426444]) = 0.00198\n",
      ">26 f([-0.00591902  0.03739604]) = 0.00143\n",
      ">27 f([-0.00634719  0.0311828 ]) = 0.00101\n",
      ">28 f([-0.00625503  0.02559933]) = 0.00069\n",
      ">29 f([-0.00573849  0.02061737]) = 0.00046\n",
      ">30 f([-0.00490679  0.01620624]) = 0.00029\n",
      ">31 f([-0.00387317  0.01233332]) = 0.00017\n",
      ">32 f([-0.00274675  0.00896449]) = 0.00009\n",
      ">33 f([-0.00162559  0.00606458]) = 0.00004\n",
      ">34 f([-0.00059149  0.00359785]) = 0.00001\n",
      ">35 f([0.0002934  0.00152838]) = 0.00000\n",
      ">36 f([ 0.00098821 -0.00017954]) = 0.00000\n",
      ">37 f([ 0.00147307 -0.00156101]) = 0.00000\n",
      ">38 f([ 0.00174746 -0.00265025]) = 0.00001\n",
      ">39 f([ 0.00182724 -0.00348028]) = 0.00002\n",
      ">40 f([ 0.00174089 -0.00408267]) = 0.00002\n",
      ">41 f([ 0.00152536 -0.00448737]) = 0.00002\n",
      ">42 f([ 0.00122173 -0.00472254]) = 0.00002\n",
      ">43 f([ 0.00087133 -0.00481438]) = 0.00002\n",
      ">44 f([ 0.00051228 -0.00478712]) = 0.00002\n",
      ">45 f([ 0.00017692 -0.00466292]) = 0.00002\n",
      ">46 f([-0.00011001 -0.00446188]) = 0.00002\n",
      ">47 f([-0.00033219 -0.004202  ]) = 0.00002\n",
      ">48 f([-0.00048176 -0.00389929]) = 0.00002\n",
      ">49 f([-0.00055861 -0.00356777]) = 0.00001\n",
      ">50 f([-0.00056912 -0.00321961]) = 0.00001\n",
      ">51 f([-0.00052452 -0.00286514]) = 0.00001\n",
      ">52 f([-0.00043908 -0.00251304]) = 0.00001\n",
      ">53 f([-0.0003283  -0.00217044]) = 0.00000\n",
      ">54 f([-0.00020731 -0.00184302]) = 0.00000\n",
      ">55 f([-8.95352320e-05 -1.53514076e-03]) = 0.00000\n",
      ">56 f([ 1.43050285e-05 -1.25002847e-03]) = 0.00000\n",
      ">57 f([ 9.67123406e-05 -9.89850279e-04]) = 0.00000\n",
      ">58 f([ 0.00015359 -0.00075587]) = 0.00000\n",
      ">59 f([ 0.00018407 -0.00054858]) = 0.00000\n",
      "Done!\n",
      "f([ 0.00018407 -0.00054858]) = 0.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NLP \r\n",
    "# bigram\r\n",
    "# tf-idf (total frequency -inverse document frequency)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# #https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c\r\n",
    "# simple random sampling\r\n",
    "sample_df = df.sample(100)\r\n",
    "\r\n",
    "# Stratified sampling\r\n",
    "\r\n",
    "# undersampling/oversampling\r\n",
    "# Sampling from common probability distributions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "https://www.geeksforgeeks.org/reservoir-sampling/\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}